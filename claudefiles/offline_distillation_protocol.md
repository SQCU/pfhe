# Claudefile: Offline Distillation Data Format & Training Protocol

## Context for Implementing Claude

You're building the data format and training protocol for a distillation-based language model training system. The key insight is:

**Tutor rollouts are expensive. Tutor prefills are cheap. Student rollouts are cheap.**

Therefore:
- Generate tutor outputs OFFLINE (once, before training)
- Log everything: tokens, top-p logits, strings
- During training, only run student rollouts online
- Load pre-computed tutor logits, compute GKD gradient
- Mix with next-token regularization on diverse corpora

This is not the standard "online distillation" setup where you query the teacher during training. This is fully offline data generation followed by online student training against cached teacher signals.

## The Two Phases

### Phase 1: Offline Data Generation (Expensive, Done Once)

```
Prompts/Tasks ──→ Tutor Model ──→ Log Everything ──→ Store
                  (Kimi K2,        (tokens,           (Parquet,
                   Claude,          logits,            HuggingFace)
                   GPT-4)           strings)
                      │
                      ▼
              Optional: Filter/Curate
              with Even Stronger Model
              (GPT-5, Claude Opus, etc.)
```

### Phase 2: Online Training (Cheap, Repeated)

```
┌──────────────────┐    ┌──────────────────┐    ┌──────────────────┐
│ Student Rollouts │    │ Cached Tutor     │    │ Regularization   │
│ (online, cheap)  │    │ Logits (offline) │    │ Corpora          │
└────────┬─────────┘    └────────┬─────────┘    └────────┬─────────┘
         │                       │                       │
         └───────────┬───────────┘                       │
                     │                                   │
                     ▼                                   │
              GKD Gradient                               │
         (student vs tutor dist)                         │
                     │                                   │
                     └─────────────┬─────────────────────┘
                                   │
                                   ▼
                          Combined Gradient
                     (GKD + next-token prediction)
                                   │
                                   ▼
                            backward, step
```

## Data Format Specification

### Core Storage Unit: DistillationExample

```python
from dataclasses import dataclass
from typing import Optional
import numpy as np

@dataclass
class SparseLogits:
    """
    Sparse representation of logits at a single position.
    Only stores top-p or top-k tokens to save space.
    """
    token_ids: np.ndarray      # shape: (num_stored,), dtype: int32
    logit_values: np.ndarray   # shape: (num_stored,), dtype: float16
    
    # Metadata about what was stored
    coverage: float            # What probability mass is covered (e.g., 0.95 for top-p=0.95)
    method: str                # "top_p" or "top_k"
    threshold: float           # The p or k value used

@dataclass
class DistillationExample:
    """
    A single example for distillation training.
    """
    # === Identity ===
    example_id: str                          # Unique identifier
    source: str                              # Where this came from ("mmlu_icr", "arxiv", etc.)
    
    # === The Sequence ===
    token_ids: list[int]                     # Token IDs as generated by tutor
    text: str                                # String representation
    
    # === Tutor Logits (The Distillation Signal) ===
    logits: list[SparseLogits]               # One per position in sequence
    # len(logits) == len(token_ids)
    # logits[i] is the distribution BEFORE generating token_ids[i]
    # i.e., logits[i] = P(next_token | token_ids[:i])
    
    # === Generation Metadata ===
    tutor_model: str                         # "kimi-k2-int4", "claude-3-opus", etc.
    tutor_tokenizer: str                     # Tokenizer identifier (for cross-tokenizer alignment)
    generation_config: dict                  # temperature, top_p, top_k, etc.
    
    # === Task-Specific Fields (Optional) ===
    # For ICR-transformed benchmarks:
    context: Optional[str] = None            # The context passage
    question: Optional[str] = None           # The question asked
    answer: Optional[str] = None             # The correct answer
    reasoning_trace: Optional[str] = None    # Tutor's reasoning (may be part of text)
    
    # For other tasks:
    task_type: Optional[str] = None          # "icr", "reasoning", "code", etc.
    task_metadata: Optional[dict] = None     # Task-specific info
    
    # === Quality Signals ===
    verified: bool = False                   # Did this pass verification?
    filter_scores: Optional[dict] = None     # Scores from filtering models
    # e.g., {"gpt4_quality": 0.92, "factual_consistency": 0.87}
```

### Storage Format: Parquet with Nested Structures

```python
import pyarrow as pa
import pyarrow.parquet as pq

# Schema for efficient storage
distillation_schema = pa.schema([
    ("example_id", pa.string()),
    ("source", pa.string()),
    
    # Sequence data
    ("token_ids", pa.list_(pa.int32())),
    ("text", pa.string()),
    
    # Sparse logits - stored as struct arrays
    ("logits", pa.list_(pa.struct([
        ("token_ids", pa.list_(pa.int32())),
        ("logit_values", pa.list_(pa.float16())),
        ("coverage", pa.float32()),
        ("method", pa.string()),
        ("threshold", pa.float32()),
    ]))),
    
    # Metadata
    ("tutor_model", pa.string()),
    ("tutor_tokenizer", pa.string()),
    ("generation_config", pa.string()),  # JSON serialized
    
    # Task fields
    ("context", pa.string()),
    ("question", pa.string()),
    ("answer", pa.string()),
    ("task_type", pa.string()),
    
    # Quality
    ("verified", pa.bool_()),
    ("filter_scores", pa.string()),  # JSON serialized
])
```

### File Organization

```
distillation_data/
├── metadata.json                    # Dataset-level metadata
├── mmlu_icr/
│   ├── shard_0000.parquet
│   ├── shard_0001.parquet
│   └── ...
├── arxiv_reasoning/
│   ├── shard_0000.parquet
│   └── ...
├── code_traces/
│   └── ...
└── regularization/
    ├── wikipedia/
    │   └── ...
    └── books/
        └── ...
```

### Size Estimates

For a single example with:
- Sequence length: 512 tokens
- Top-p coverage: 0.95 (typically ~50-200 tokens per position)
- Average stored tokens per position: 100

Storage per example:
- token_ids: 512 × 4 bytes = 2 KB
- text: ~2 KB (assuming ~4 chars/token)
- logits: 512 positions × 100 tokens × (4 + 2) bytes = 300 KB
- metadata: ~1 KB

**Total: ~305 KB per example**

For 1M examples: ~300 GB
For 10M examples: ~3 TB

This is large but tractable. Can reduce by:
- Lower top-p (0.90 instead of 0.95)
- Quantize logits further (int8 instead of float16)
- Only store logits for "interesting" positions (high entropy)

## Logit Logging Protocol

### During Tutor Generation

```python
def generate_with_logits(
    model,
    tokenizer,
    prompt: str,
    max_tokens: int,
    temperature: float = 1.0,
    top_p_storage: float = 0.95,  # How much probability mass to store
) -> DistillationExample:
    """
    Generate from tutor model while logging logits.
    """
    input_ids = tokenizer.encode(prompt)
    generated_ids = []
    logits_list = []
    
    for _ in range(max_tokens):
        # Forward pass
        outputs = model(input_ids + generated_ids, return_logits=True)
        next_token_logits = outputs.logits[-1]  # Shape: (vocab_size,)
        
        # Store sparse logits
        sparse = sparsify_logits(next_token_logits, method="top_p", threshold=top_p_storage)
        logits_list.append(sparse)
        
        # Sample next token
        probs = softmax(next_token_logits / temperature)
        next_token = sample(probs, top_p=top_p)
        generated_ids.append(next_token)
        
        # Check for EOS
        if next_token == tokenizer.eos_token_id:
            break
    
    return DistillationExample(
        token_ids=generated_ids,
        text=tokenizer.decode(generated_ids),
        logits=logits_list,
        tutor_model=model.name,
        tutor_tokenizer=tokenizer.name,
        generation_config={"temperature": temperature, "top_p": top_p},
    )

def sparsify_logits(logits: np.ndarray, method: str, threshold: float) -> SparseLogits:
    """
    Convert dense logits to sparse representation.
    """
    if method == "top_p":
        probs = softmax(logits)
        sorted_indices = np.argsort(probs)[::-1]
        sorted_probs = probs[sorted_indices]
        cumsum = np.cumsum(sorted_probs)
        cutoff = np.searchsorted(cumsum, threshold) + 1
        
        top_indices = sorted_indices[:cutoff]
        top_logits = logits[top_indices]
        
        return SparseLogits(
            token_ids=top_indices.astype(np.int32),
            logit_values=top_logits.astype(np.float16),
            coverage=float(cumsum[cutoff - 1]),
            method="top_p",
            threshold=threshold,
        )
    
    elif method == "top_k":
        k = int(threshold)
        top_indices = np.argpartition(logits, -k)[-k:]
        top_logits = logits[top_indices]
        
        probs = softmax(logits)
        coverage = probs[top_indices].sum()
        
        return SparseLogits(
            token_ids=top_indices.astype(np.int32),
            logit_values=top_logits.astype(np.float16),
            coverage=float(coverage),
            method="top_k",
            threshold=threshold,
        )
```

### Batch Generation with vLLM

For efficient generation at scale:

```python
from vllm import LLM, SamplingParams

def batch_generate_with_logits(
    model_name: str,
    prompts: list[str],
    max_tokens: int,
    top_p_storage: float = 0.95,
) -> list[DistillationExample]:
    """
    Generate many examples efficiently using vLLM.
    
    Note: vLLM doesn't natively return full logits for all positions.
    Options:
    1. Use logprobs parameter (returns top-k logprobs)
    2. Modify vLLM to return full logits
    3. Run a second "prefill-only" pass to get logits
    """
    llm = LLM(model=model_name)
    
    # vLLM's logprobs parameter gives us top-k logprobs per token
    sampling_params = SamplingParams(
        max_tokens=max_tokens,
        temperature=1.0,
        logprobs=100,  # Get top-100 logprobs per position
    )
    
    outputs = llm.generate(prompts, sampling_params)
    
    examples = []
    for output in outputs:
        token_ids = output.outputs[0].token_ids
        logprobs = output.outputs[0].logprobs  # List of dicts: token_id -> logprob
        
        # Convert logprobs to SparseLogits
        logits_list = []
        for lp_dict in logprobs:
            token_ids_sparse = np.array(list(lp_dict.keys()), dtype=np.int32)
            # Convert logprobs back to logits (unnormalized)
            logit_values = np.array(list(lp_dict.values()), dtype=np.float16)
            
            logits_list.append(SparseLogits(
                token_ids=token_ids_sparse,
                logit_values=logit_values,
                coverage=0.0,  # Unknown without full distribution
                method="top_k",
                threshold=100,
            ))
        
        examples.append(DistillationExample(
            token_ids=list(token_ids),
            text=output.outputs[0].text,
            logits=logits_list,
            tutor_model=model_name,
            # ... etc
        ))
    
    return examples
```

## Training Protocol

### The GKD Loss with Cached Logits

```python
def gkd_loss(
    student_logits: torch.Tensor,      # (batch, seq_len, vocab_size)
    tutor_sparse_logits: list[list[SparseLogits]],  # batch x seq_len
    divergence: str = "forward_kl",    # "forward_kl", "reverse_kl", "jsd"
    jsd_beta: float = 0.1,             # For JSD
) -> torch.Tensor:
    """
    Compute GKD loss between student logits and cached tutor logits.
    
    The tutor logits are sparse (only top-p tokens stored), so we need
    to handle the missing probability mass.
    """
    batch_size, seq_len, vocab_size = student_logits.shape
    total_loss = 0.0
    
    for b in range(batch_size):
        for t in range(seq_len):
            sparse = tutor_sparse_logits[b][t]
            
            # Reconstruct tutor distribution (sparse)
            tutor_logits_dense = torch.full((vocab_size,), float('-inf'), device=student_logits.device)
            tutor_logits_dense[sparse.token_ids] = torch.tensor(sparse.logit_values, device=student_logits.device)
            
            # Get student distribution
            student_logits_t = student_logits[b, t]
            
            if divergence == "forward_kl":
                # KL(tutor || student) - tutor is the "target"
                # Only sum over tokens where tutor has support
                tutor_probs = F.softmax(tutor_logits_dense, dim=-1)
                student_log_probs = F.log_softmax(student_logits_t, dim=-1)
                
                # Mask to tutor's support
                mask = tutor_probs > 0
                loss_t = (tutor_probs[mask] * (tutor_probs[mask].log() - student_log_probs[mask])).sum()
            
            elif divergence == "reverse_kl":
                # KL(student || tutor) - student is the "target"
                student_probs = F.softmax(student_logits_t, dim=-1)
                tutor_log_probs = F.log_softmax(tutor_logits_dense, dim=-1)
                
                # This is tricky because tutor has -inf for unsupported tokens
                # Option: only compute over tutor's support, or add smoothing
                mask = tutor_logits_dense > float('-inf')
                loss_t = (student_probs[mask] * (student_probs[mask].log() - tutor_log_probs[mask])).sum()
            
            elif divergence == "jsd":
                # JSD interpolates between forward and reverse KL
                tutor_probs = F.softmax(tutor_logits_dense, dim=-1)
                student_probs = F.softmax(student_logits_t, dim=-1)
                
                # Mixture distribution
                m = jsd_beta * tutor_probs + (1 - jsd_beta) * student_probs
                
                loss_t = jsd_beta * F.kl_div(m.log(), tutor_probs, reduction='sum')
                loss_t += (1 - jsd_beta) * F.kl_div(m.log(), student_probs, reduction='sum')
            
            total_loss += loss_t
    
    return total_loss / (batch_size * seq_len)
```

### The Combined Training Step

```python
def training_step(
    student_model,
    optimizer,
    
    # Distillation batch
    distill_batch: list[DistillationExample],
    
    # Regularization batch (standard next-token prediction)
    regularization_tokens: torch.Tensor,  # (batch, seq_len)
    
    # Hyperparameters
    distill_weight: float = 1.0,
    regularization_weight: float = 1.0,
    divergence: str = "jsd",
    jsd_beta: float = 0.1,
):
    """
    Combined training step: GKD distillation + next-token regularization.
    """
    optimizer.zero_grad()
    
    # === Distillation Loss ===
    # Convert distill batch to tensors
    distill_tokens = torch.tensor([ex.token_ids for ex in distill_batch])
    
    # Forward pass on distillation sequences
    student_logits = student_model(distill_tokens).logits
    
    # Compute GKD loss against cached tutor logits
    distill_loss = gkd_loss(
        student_logits=student_logits,
        tutor_sparse_logits=[ex.logits for ex in distill_batch],
        divergence=divergence,
        jsd_beta=jsd_beta,
    )
    
    # === Regularization Loss ===
    # Standard next-token prediction on diverse corpora
    reg_logits = student_model(regularization_tokens[:, :-1]).logits
    reg_targets = regularization_tokens[:, 1:]
    
    regularization_loss = F.cross_entropy(
        reg_logits.reshape(-1, reg_logits.size(-1)),
        reg_targets.reshape(-1),
    )
    
    # === Combined Loss ===
    total_loss = distill_weight * distill_loss + regularization_weight * regularization_loss
    
    # Backward and step
    total_loss.backward()
    optimizer.step()
    
    return {
        "loss/total": total_loss.item(),
        "loss/distill": distill_loss.item(),
        "loss/regularization": regularization_loss.item(),
    }
```

### The Training Loop

```python
def train(
    student_model,
    distillation_dataset,      # The offline-generated distillation data
    regularization_dataset,    # Diverse corpora for regularization
    
    num_steps: int,
    batch_size: int,
    distill_weight: float = 1.0,
    regularization_weight: float = 1.0,
    
    # Curriculum
    curriculum_schedule: Callable[[int], str] = None,  # step -> dataset_subset
):
    """
    Full training loop with offline distillation.
    """
    optimizer = torch.optim.AdamW(student_model.parameters(), lr=1e-4)
    
    distill_loader = DataLoader(distillation_dataset, batch_size=batch_size, shuffle=True)
    reg_loader = DataLoader(regularization_dataset, batch_size=batch_size, shuffle=True)
    
    distill_iter = iter(distill_loader)
    reg_iter = iter(reg_loader)
    
    for step in range(num_steps):
        # Get batches (with cycling)
        try:
            distill_batch = next(distill_iter)
        except StopIteration:
            distill_iter = iter(distill_loader)
            distill_batch = next(distill_iter)
        
        try:
            reg_batch = next(reg_iter)
        except StopIteration:
            reg_iter = iter(reg_loader)
            reg_batch = next(reg_iter)
        
        # Training step
        metrics = training_step(
            student_model=student_model,
            optimizer=optimizer,
            distill_batch=distill_batch,
            regularization_tokens=reg_batch["input_ids"],
            distill_weight=distill_weight,
            regularization_weight=regularization_weight,
        )
        
        # Logging
        if step % 100 == 0:
            print(f"Step {step}: {metrics}")
        
        # Optional: adjust curriculum
        if curriculum_schedule is not None:
            current_subset = curriculum_schedule(step)
            # Switch to appropriate data subset
            # (implementation depends on how curriculum is structured)
```

## On-Policy Student Rollouts

The above shows teacher-forcing on the distillation sequences. For true GKD, we want on-policy student rollouts:

```python
def on_policy_training_step(
    student_model,
    optimizer,
    
    # Prompts for student generation
    prompts: list[str],
    tokenizer,
    
    # Cached tutor data (keyed by prompt or task)
    tutor_cache: dict[str, DistillationExample],
    
    # Regularization
    regularization_tokens: torch.Tensor,
    
    # Config
    max_gen_tokens: int = 256,
    temperature: float = 1.0,
):
    """
    On-policy GKD: student generates, then we compare to cached tutor logits.
    
    Key insight: the student's generation may differ from the tutor's.
    We need to re-run tutor prefill on student's sequence to get comparable logits.
    
    BUT we're trying to avoid online tutor inference!
    
    Options:
    1. Only compare on the prompt portion (where sequences match)
    2. Use the cached tutor logits as a "soft target" regardless of student's actual generation
    3. Accept that on-policy requires some online tutor inference (but cheaper than full generation)
    
    For now: Option 2 (use cached logits as soft target on student's sequence)
    This is an approximation but avoids online tutor inference.
    """
    optimizer.zero_grad()
    
    # Generate from student
    prompt_ids = [tokenizer.encode(p) for p in prompts]
    student_generations = []
    
    with torch.no_grad():
        for pid in prompt_ids:
            generated = student_model.generate(
                torch.tensor([pid]),
                max_new_tokens=max_gen_tokens,
                temperature=temperature,
                do_sample=True,
            )
            student_generations.append(generated[0].tolist())
    
    # Now compute loss
    # For each student generation, get the cached tutor logits for that prompt
    # and use them as soft targets (even though sequences may differ)
    
    distill_loss = 0.0
    for prompt, student_seq in zip(prompts, student_generations):
        tutor_example = tutor_cache.get(prompt)
        if tutor_example is None:
            continue
        
        # Forward pass on student's sequence
        student_logits = student_model(torch.tensor([student_seq])).logits[0]
        
        # Compare to tutor logits (position by position, up to min length)
        min_len = min(len(student_seq), len(tutor_example.logits))
        for t in range(min_len):
            # ... same GKD loss computation as before ...
            pass
    
    # Add regularization loss
    # ... same as before ...
    
    total_loss.backward()
    optimizer.step()
```

## Cross-Tokenizer Alignment

When tutor and student use different tokenizers:

```python
def align_tokenizations(
    text: str,
    tutor_tokenizer,
    student_tokenizer,
    tutor_logits: list[SparseLogits],
) -> list[SparseLogits]:
    """
    Align tutor logits to student tokenization.
    
    The key insight: we're aligning the SAME STRING under two deterministic tokenizations.
    This is a computable mapping.
    """
    # Get both tokenizations with offset mappings
    tutor_encoding = tutor_tokenizer(text, return_offsets_mapping=True)
    student_encoding = student_tokenizer(text, return_offsets_mapping=True)
    
    tutor_offsets = tutor_encoding.offset_mapping  # List of (start, end) char positions
    student_offsets = student_encoding.offset_mapping
    
    # For each student token, find overlapping tutor tokens
    aligned_logits = []
    
    for s_idx, (s_start, s_end) in enumerate(student_offsets):
        # Find tutor tokens that overlap with this student token
        overlapping_tutor_indices = []
        for t_idx, (t_start, t_end) in enumerate(tutor_offsets):
            if t_start < s_end and t_end > s_start:  # Overlap
                overlapping_tutor_indices.append(t_idx)
        
        if not overlapping_tutor_indices:
            # No overlap (shouldn't happen for same string)
            aligned_logits.append(None)
            continue
        
        # Aggregate tutor logits for overlapping tokens
        # Strategy: average the logits (in log space)
        aggregated = aggregate_sparse_logits(
            [tutor_logits[i] for i in overlapping_tutor_indices],
            method="mean",
        )
        
        # Map tutor token IDs to student vocab
        # This requires a token-to-string-to-token mapping
        mapped = map_sparse_logits_to_vocab(
            aggregated,
            tutor_tokenizer,
            student_tokenizer,
        )
        
        aligned_logits.append(mapped)
    
    return aligned_logits

def map_sparse_logits_to_vocab(
    sparse: SparseLogits,
    source_tokenizer,
    target_tokenizer,
) -> SparseLogits:
    """
    Map sparse logits from one vocabulary to another.
    
    For each source token, decode to string, re-encode with target tokenizer.
    Distribute probability mass accordingly.
    """
    # This is approximate because:
    # 1. One source token may map to multiple target tokens (or vice versa)
    # 2. We're working with logits, not probabilities
    
    # Convert to probabilities for easier manipulation
    probs = softmax(sparse.logit_values)
    
    target_probs = defaultdict(float)
    
    for tok_id, prob in zip(sparse.token_ids, probs):
        # Decode source token to string
        tok_str = source_tokenizer.decode([tok_id])
        
        # Encode with target tokenizer
        target_ids = target_tokenizer.encode(tok_str, add_special_tokens=False)
        
        # Distribute probability (evenly for now; could be smarter)
        for tid in target_ids:
            target_probs[tid] += prob / len(target_ids)
    
    # Convert back to sparse logits
    target_ids = np.array(list(target_probs.keys()), dtype=np.int32)
    target_probs_arr = np.array(list(target_probs.values()), dtype=np.float32)
    target_logits = np.log(target_probs_arr + 1e-10).astype(np.float16)
    
    return SparseLogits(
        token_ids=target_ids,
        logit_values=target_logits,
        coverage=sparse.coverage,
        method="mapped",
        threshold=0,
    )
```

## The Regularization Insight

You mentioned that mixing GKD gradients with next-token prediction gradients may obviate the need for explicit KL regularization toward a reference policy.

The intuition:
- Standard RLHF/GKD: `loss = task_loss - β * KL(policy || reference)`
- The KL term prevents collapse to only optimizing the task
- Our approach: `loss = α * distill_loss + (1-α) * next_token_loss`
- The next-token loss on diverse corpora serves the same purpose
- The model can't collapse to only matching the tutor because it's also being trained on diverse text

This is **implicit regularization** rather than explicit KL penalty.

Benefits:
- Simpler loss function
- No need to maintain a frozen reference model
- Regularization strength is controlled by data mixture, not hyperparameter
- More natural curriculum: adjust regularization by changing data proportions

The key is that the regularization corpora must be:
1. Diverse (not just more of the same task distribution)
2. High quality (not noise)
3. Properly weighted relative to distillation data

## CLI Interface

```bash
# Generate distillation data from prompts
python -m distillation generate \
    --prompts prompts.jsonl \
    --tutor-model kimi-k2 \
    --output-dir ./distillation_data/ \
    --top-p-storage 0.95

# Convert existing dataset to distillation format (add logits via prefill)
python -m distillation add-logits \
    --input-data existing_data.jsonl \
    --tutor-model kimi-k2 \
    --output-dir ./distillation_data/

# Train student with offline distillation
python -m distillation train \
    --student-model ./checkpoints/student_init/ \
    --distillation-data ./distillation_data/ \
    --regularization-data ./regularization_corpora/ \
    --output-dir ./checkpoints/student_trained/ \
    --distill-weight 1.0 \
    --reg-weight 1.0 \
    --divergence jsd \
    --num-steps 100000

# Evaluate student on ICR benchmarks
python -m distillation evaluate \
    --model ./checkpoints/student_trained/ \
    --benchmarks mmlu_icr,triviaqa_icr \
    --output results.json
```

## Dependencies

```
# Data handling
pyarrow            # Parquet storage
datasets           # HuggingFace datasets integration
numpy

# Inference
vllm               # Fast tutor inference
transformers       # Model loading

# Training
torch
accelerate         # Distributed training
```

## Success Criteria

1. **Data generation**: Successfully generate and store distillation data with logits
2. **Training convergence**: Student loss decreases on both distillation and regularization
3. **Eval improvement**: Student improves on ICR benchmarks over training
4. **Efficiency**: Training is not bottlenecked by data loading (offline data is fast)
5. **Cross-tokenizer**: Alignment works for common tokenizer pairs (Llama ↔ GPT ↔ etc.)

---

When done, this system should enable fully offline distillation from expensive tutor models, with online training using only cheap student inference.
