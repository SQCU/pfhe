# PHFE Project Progress Review

**Date**: 2025-12-22
**Status**: Phase 1 Infrastructure (Substantial Progress)

---

## Project Goals Summary

PHFE (Posthumanity's First Exam) is a benchmark suite measuring **in-context learning** vs **memorization**. The core scientific claim:

> "A model trained on synthetic problems (verified non-overlapping with canonical benchmarks) achieves X% on canonical eval and Y% on ICR-augmented eval. The gap (Y - X) represents pure in-context learning capability, not contamination."

---

## Specification Checklist

### Core Specifications

| Spec File | Purpose | Implementation Status |
|-----------|---------|----------------------|
| `pfe_benchmark_spec.md` | Benchmark suite, three-split architecture, contamination firewall | **Substantial** |
| `answer_key_corpus.md` | Multi-tutor answer key collection, style diversity | Stub only |
| `offline_distillation_protocol.md` | GKD training with cached tutor logits | **Cross-tokenizer core done** |

### Supporting Specifications

| Spec File | Purpose | Implementation Status |
|-----------|---------|----------------------|
| `benchmark_icr_transform.md` | ICR transformation methodology | **Complete** |
| `arxiv_tex_pipeline.md` | LaTeX â†’ multimodal training data | Stub only |
| `curriculum_generator.md` | Procedural problem generation (superseded) | Stub only |
| `language_eval_suite.md` | Language competence metrics | Partial (repetition only) |

---

## Phase 1: Infrastructure â€” Detailed Status

### Completed âœ…

#### 1. Project Structure & Tooling
- [x] `uv` environment with Python 3.10
- [x] `pyproject.toml` with all dependencies (ruff, mypy, pytest, sentence-transformers, etc.)
- [x] Source layout: `src/phfe/` with module packages
- [x] Test infrastructure: `tests/` with pytest

#### 2. Orchestrator Infrastructure (`src/phfe/orchestrator/`)
- [x] `observability.py`: `SubagentLog`, `WorkflowTrace`, `TraceStore`, `Timer`
- [x] `tutor.py`: `TutorCaller` with OpenAI/Anthropic/vLLM backends
- [x] Sparse logit storage format (`SparseLogits` dataclass)
- [x] Cost tracking per API call

#### 3. Contamination Firewall (`src/phfe/benchmark/contamination.py`)
- [x] **Token-level overlap**: n-gram Jaccard similarity (5-grams, 30% threshold)
- [x] **Semantic similarity**: Sentence-transformer embeddings (85% cosine threshold)
- [x] **Math structural**: Same numbers + operations + answer detection
- [x] **Code structural**: Same function name + test inputs detection
- [x] Combined `ContaminationFirewall` class
- [x] Batch indexing for efficiency
- [x] Statistics tracking (pass rate, rejection reasons)
- [x] **37/37 tests passing**

#### 4. CLI Framework (`src/phfe/cli.py`)
- [x] Typer-based CLI with subcommands
- [x] `phfe status` - show project status
- [x] `phfe benchmark list` - list benchmarks
- [x] Stub commands for all major operations

#### 5. Benchmark Loading (`src/phfe/benchmark/loader.py`) âœ… NEW
- [x] `BenchmarkLoader` class with HuggingFace datasets integration
- [x] `BenchmarkConfig` dataclass for per-benchmark configuration
- [x] Configs for: GSM8K, ARC-Easy/Challenge, RACE, BoolQ, HellaSwag, WinoGrande, MBPP
- [x] Field extraction, options parsing, answer type handling
- [x] Caching and limit support
- [x] **23/23 tests passing**

#### 6. ICR Transformation (`src/phfe/icr_transform/`) âœ… NEW
- [x] `method_libraries.py`: Method libraries for all 10+ benchmarks
  - GSM1K: Arithmetic word problem methods (6 methods)
  - ARC: Science fact library (5 categories + worked examples)
  - RACE: Reading comprehension strategies (6 strategies)
  - BoolQ: Yes/no question strategies (4 approaches)
  - HellaSwag: Commonsense completion principles (5 principles)
  - WinoGrande: Coreference resolution strategies (4 strategies)
  - MBPP: Programming pattern library (6 patterns)
  - FnCall: Function calling guide (6 example functions)
  - Format: Data transformation patterns (6 patterns)
- [x] `ICRTransformer` class for prepending method libraries
- [x] `ICRInstance` and `ICRBatch` dataclasses
- [x] Library versioning via content hashing
- [x] **39/39 tests passing**

#### 7. Cross-Tokenizer Distillation (`src/phfe/distillation/cross_tokenizer.py`) âœ… NEW
- [x] `SparseLogits`: Top-p sparse logit representation
- [x] `CrossTokenizerAligner`: Character-span-based token alignment
- [x] `LogitAggregator`: Multiple aggregation strategies (first, average, max)
- [x] `VocabularyMapper`: Exact/normalized token mapping between vocabs
- [x] `compute_gkd_loss`: Forward KL loss computation
- [x] **21/21 tests passing**

#### 8. Cross-Tokenizer GKD Testbed (`scripts/cross_tokenizer_testbed.py`) âœ… NEW
- [x] Tested with real models: Qwen2-0.5B (student) + Pythia-1B (teacher)
- [x] **100% alignment coverage** despite different tokenizers (151K vs 50K vocab)
- [x] **82.8% vocabulary mapping** between tokenizers
- [x] Real GKD loss computation verified:
  - "Hello world!": 8.85 nats
  - Complex text: 11-14 nats (expected divergence)
- [x] GPU inference working with device_map="auto"

#### 9. Task Queue Orchestrator (`src/phfe/orchestrator/task_queue.py`) âœ… NEW
- [x] `TaskQueue` class with uniform interface for all worker types
- [x] Supports: Claude Code subagents, external scripts, vLLM servers
- [x] Queue types: generate, contaminate_check, tutor_inference, verify_answer, icr_transform
- [x] Worker concern levels: info, review, retry, error, escalate
- [x] JSONL persistence for crash recovery
- [x] Compact reporting for Claude Code orchestration
- [x] **42/42 tests passing**

#### 10. Claude Code Agent Definitions (`.claude/agents/`) âœ… NEW
- [x] `problem-generator.md` â€” Generate synthetic problems via task queue
- [x] `answer-verifier.md` â€” Verify tutor answers (Opus-tier)
- [x] `contamination-reviewer.md` â€” Review edge-case contamination flags

### In Progress ðŸ”„

#### 9. Tutor Inference Setup
- [ ] vLLM server configuration for open-weight models
- [ ] API client configuration (OpenAI, Anthropic)
- [ ] Logit capture during generation

#### 9. Answer Key Logging
- [ ] Token + logit storage in Parquet format
- [ ] Multi-tutor collection pipeline
- [ ] Style diversity enforcement (<40% per tutor)

---

## Phase 2: Corpus Generation â€” Not Started

| Task | Status |
|------|--------|
| Generate synthetic problems | âŒ |
| Contamination checking at generation time | âŒ |
| Collect answer keys from tutor ensemble | âŒ |
| Quality filtering and verification | âŒ |
| Assemble final corpus (39K problems, 100K+ answer keys) | âŒ |

---

## Phase 3: Training â€” Partial

| Task | Status |
|------|--------|
| Implement GKD training loop | âŒ |
| Cross-tokenizer alignment | âœ… Core logic done |
| Regularization mixing | âŒ |
| Train student model | âŒ |

---

## Phase 4: Evaluation â€” Not Started

| Task | Status |
|------|--------|
| Canonical benchmark evaluation | âŒ |
| ICR-augmented evaluation | âŒ |
| Baseline comparisons | âŒ |
| Report generation | âŒ |

---

## Benchmark Coverage

### Target Benchmarks

| Benchmark | Canonical Size | Synthetic Target | Domain | Status |
|-----------|---------------|------------------|--------|--------|
| GSM1K | 1,250 | 10,000 | math | Loader + ICR ready |
| ARC-Easy | 2,376 | 2,500 | science | Loader + ICR ready |
| ARC-Challenge | 1,172 | 2,500 | science | Loader + ICR ready |
| RACE | 4,934 | 5,000 | reading | Loader + ICR ready |
| BoolQ | 3,270 | 5,000 | boolean | Loader + ICR ready |
| HellaSwag | 10,042 | 5,000 | commonsense | Loader + ICR ready |
| WinoGrande | 1,267 | 3,000 | coreference | Loader + ICR ready |
| MBPP | 500 | 2,000 | code | Loader + ICR ready |
| FnCall | 500 | 2,000 | code | ICR ready (no HF dataset) |
| Format | 500 | 2,000 | code | ICR ready (no HF dataset) |

**Total Canonical**: ~25,000 problems
**Total Synthetic Target**: ~39,000 problems

---

## Tutor Ensemble

| Model | Access | Role | Integration Status |
|-------|--------|------|-------------------|
| DeepSeek-R1 | Weights | Reasoning traces | TutorCaller stub |
| Kimi K2 | Weights | Math strength | TutorCaller stub |
| Qwen-72B | Weights | General capability | TutorCaller stub |
| GPT-4o | API | High accuracy baseline | TutorCaller ready |
| Claude Sonnet | API | Clear explanations | TutorCaller ready |

---

## Test Coverage

```
Total: 162 tests passing

tests/test_contamination.py ........... 37 passed
tests/test_benchmark_loader.py ........ 23 passed
tests/test_icr_transform.py ........... 39 passed
tests/test_cross_tokenizer.py ......... 21 passed
tests/test_task_queue.py .............. 42 passed
```

| Component | Tests | Status |
|-----------|-------|--------|
| Tokenization & n-grams | 6 | âœ… |
| Token overlap checker | 4 | âœ… |
| Math structural checker | 4 | âœ… |
| Code structural checker | 4 | âœ… |
| Semantic similarity | 2 | âœ… |
| Combined firewall | 6 | âœ… |
| Convenience functions | 2 | âœ… |
| Edge cases | 4 | âœ… |
| Benchmark configs | 5 | âœ… |
| Loader init | 2 | âœ… |
| Mock loading | 8 | âœ… |
| Error handling | 2 | âœ… |
| Method libraries | 8 | âœ… |
| ICR transformer | 5 | âœ… |
| Transform single | 5 | âœ… |
| Transform batch | 5 | âœ… |
| All benchmarks | 11 | âœ… |
| Sparse logits | 5 | âœ… |
| Cross-tokenizer aligner | 4 | âœ… |
| Logit aggregator | 4 | âœ… |
| Vocabulary mapper | 3 | âœ… |
| GKD loss | 4 | âœ… |
| Task queue basics | 5 | âœ… |
| Claim and submit | 6 | âœ… |
| Worker concerns | 5 | âœ… |
| Queue status | 6 | âœ… |
| Retry and progress | 3 | âœ… |
| Debug mode | 1 | âœ… |
| Persistence | 3 | âœ… |
| Serialization | 2 | âœ… |
| Convenience functions | 1 | âœ… |
| Queue types | 5 | âœ… |
| List tasks | 4 | âœ… |
| Concern summary | 1 | âœ… |

---

## Files Created

```
src/phfe/
â”œâ”€â”€ __init__.py                      # Package root
â”œâ”€â”€ cli.py                           # Typer CLI
â”œâ”€â”€ orchestrator/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ observability.py             # Tracing infrastructure
â”‚   â”œâ”€â”€ tutor.py                     # Multi-model caller
â”‚   â””â”€â”€ task_queue.py                # ðŸ†• Task queue orchestrator
â”œâ”€â”€ benchmark/
â”‚   â”œâ”€â”€ __init__.py                  # Benchmark types, exports
â”‚   â”œâ”€â”€ contamination.py             # Contamination firewall
â”‚   â””â”€â”€ loader.py                    # HuggingFace loader
â”œâ”€â”€ icr_transform/
â”‚   â”œâ”€â”€ __init__.py                  # ICR transformer
â”‚   â””â”€â”€ method_libraries.py          # All benchmark method libraries
â”œâ”€â”€ distillation/
â”‚   â”œâ”€â”€ __init__.py                  # GKD training stubs
â”‚   â””â”€â”€ cross_tokenizer.py           # Cross-tokenizer alignment
â”œâ”€â”€ answer_key_corpus/
â”‚   â””â”€â”€ __init__.py                  # Answer key collection stubs
â”œâ”€â”€ arxiv_pipeline/
â”‚   â””â”€â”€ __init__.py                  # LaTeX pipeline stubs
â”œâ”€â”€ curriculum_generator/
â”‚   â””â”€â”€ __init__.py                  # Problem generation stubs
â””â”€â”€ language_evals/
    â””â”€â”€ __init__.py                  # Language competence stubs

.claude/agents/                       # ðŸ†• Claude Code agent definitions
â”œâ”€â”€ problem-generator.md             # Synthetic problem generation
â”œâ”€â”€ answer-verifier.md               # Answer verification (Opus)
â””â”€â”€ contamination-reviewer.md        # Edge-case contamination review

tests/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ test_contamination.py            # 37 tests
â”œâ”€â”€ test_benchmark_loader.py         # 23 tests
â”œâ”€â”€ test_icr_transform.py            # 39 tests
â”œâ”€â”€ test_cross_tokenizer.py          # 21 tests
â””â”€â”€ test_task_queue.py               # ðŸ†• 42 tests

scripts/
â””â”€â”€ cross_tokenizer_testbed.py       # GKD testbed with real models

claudefiles/
â””â”€â”€ task_orchestrator_spec.md        # ðŸ†• Task orchestrator specification
```

---

## Next Steps â€” Suggested Priorities

### Option A: Complete Tutor Inference (vLLM)
1. **vLLM Setup** â€” Configure vLLM for logprob extraction
2. **Logit Capture** â€” Extract top-p sparse logits during generation
3. **Multi-tutor Pipeline** â€” Route to different teachers based on domain

### Option B: Cross-Tokenizer GKD Testbed
1. **Load Qwen-1B and Gemma-4B** â€” Download and configure
2. **Alignment Test** â€” Verify cross-tokenizer alignment works
3. **Loss Measurement** â€” Compute GKD loss without gradients

### Option C: Synthetic Generation Pipeline
1. **Problem Generator** â€” Use tutors to generate synthetic problems
2. **Generation + Firewall Loop** â€” Generate, check, accept/reject
3. **Answer Key Collection** â€” Multi-tutor answer keys with logits

---

## Reference Implementation

The `reference_dialogue_yoinker/` directory contains patterns from a similar subagent orchestrator project:
- `subagent_orchestrator/subagent.py` â€” API caller pattern
- `subagent_orchestrator/observability.py` â€” Trace storage pattern
- `api_server.py` â€” FastAPI dispatch example

---

## Commands

```bash
# Sync dependencies
uv sync --extra dev

# Run tests
uv run pytest tests/ -v

# Run specific test file
uv run pytest tests/test_cross_tokenizer.py -v

# Check project status
uv run phfe status

# List benchmarks
uv run phfe benchmark list

# Load a benchmark (example)
uv run python -c "from phfe.benchmark import load_benchmark, BenchmarkType; print(load_benchmark(BenchmarkType.GSM1K, limit=3))"
```
